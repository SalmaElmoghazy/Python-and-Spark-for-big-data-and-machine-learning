{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9da32d6",
   "metadata": {},
   "source": [
    "# NLP Using PySpark - Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8326ba88",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "- The objective from this project is to create a <b>Spam filter using NaiveBayes classifier</b>.\n",
    "- It is required to obtain <b>f1_scored > 0.9</b>.\n",
    "- We'll use a dataset from UCI Repository. SMS Spam Detection: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971f788",
   "metadata": {
    "id": "gUxZnsqrmynW"
   },
   "source": [
    "## To perform this task follow the following guiding steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bc851",
   "metadata": {
    "id": "gUxZnsqrmynW"
   },
   "source": [
    "### Create a spark session and import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf86e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as fn\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7df9e",
   "metadata": {
    "id": "gUxZnsqrmynW"
   },
   "source": [
    "### Read the readme file to learn more about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d00718f",
   "metadata": {
    "id": "gUxZnsqrmynW"
   },
   "source": [
    "### Read the data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29914cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"SMSSpamCollection.csv\", header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cd7f6",
   "metadata": {
    "id": "gUxZnsqrmynW"
   },
   "source": [
    "### Print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52706b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90cce7",
   "metadata": {
    "id": "gUxZnsqrmynW"
   },
   "source": [
    "### Rename the first column to 'class' and second column to 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a88df6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, FloatType\n",
    "\n",
    "df = df.withColumnRenamed(\"_c0\",\"class\").withColumnRenamed(\"_c1\",\"text\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362dcb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe744a9",
   "metadata": {},
   "source": [
    "## Clean and Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d693167",
   "metadata": {},
   "source": [
    "### Create a new feature column contains the length of the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5424a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_len(txt):\n",
    "    return len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a8a6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenUDF = fn.udf(calc_len, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84012788",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn('length', lenUDF('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b786c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR\n",
    "df2 = df.withColumn('length', fn.length(df.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea2de6",
   "metadata": {},
   "source": [
    "### Show the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04c67c53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "+-----+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca0199f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "+-----+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e37a6",
   "metadata": {},
   "source": [
    "### Get the average text length for each class (give alias name to the average length column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c32727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      Avg. Length|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('class').agg(fn.avg('length').alias('Avg. Length')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e101af",
   "metadata": {},
   "source": [
    "## Feature Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ad9dd",
   "metadata": {},
   "source": [
    "### In this part you transform you raw text in to tf_idf model :\n",
    "- For more information about TF-IDF check the following link:\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225067d5",
   "metadata": {},
   "source": [
    "### Perform the following steps to obtain TF-IDF:\n",
    "1. Import the required transformers/estimators for the subsequent steps.\n",
    "2. Create a <b>Tokenizer</b> from the text column.\n",
    "3. Create a <b>StopWordsRemover</b> to remove the <b>stop words</b> from the column obtained from the <b>Tokenizer</b>.\n",
    "4. Create a <b>CountVectorizer</b> after removing the <b>stop words</b>.\n",
    "5. Create the <b>TF-IDF</b> from the <b>CountVectorizer</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a4eebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b82756db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer_8bd12eefc5c4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text tokenization\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.setInputCol('text')\n",
    "tokenizer.setOutputCol(\"Tokenized_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a743ff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_8f2f49756de5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stop words\n",
    "remover = StopWordsRemover()\n",
    "stopwords = remover.getStopWords() \n",
    "\n",
    "# Display default list\n",
    "print(stopwords[:10])\n",
    "\n",
    "remover.setInputCol(\"Tokenized_words\")\n",
    "remover.setOutputCol(\"Tokenized_words_no_stop_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e2916e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer_482fa74caf1d"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorization\n",
    "cv = CountVectorizer()\n",
    "cv.setInputCol(\"Tokenized_words_no_stop_words\")\n",
    "cv.setOutputCol(\"vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc8c387d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IDF_5ab72570b9c8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate scores\n",
    "idf = IDF()\n",
    "idf.setInputCol('vectors')\n",
    "idf.setOutputCol('tfidf_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527ad65",
   "metadata": {},
   "source": [
    "- Convert the <b>class column</b> to index using <b>StringIndexer</b>\n",
    "- Create feature column from the <b>TF-IDF</b> and <b>lenght</b> columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf46159",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCols=['class'],\n",
    "                              outputCols=['class_index'],\n",
    "                              handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad9d4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=['tfidf_features', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9775d494",
   "metadata": {},
   "source": [
    "## The Model\n",
    "- Create a <b>NaiveBayes</b> classifier with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57af0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c7384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayes(featuresCol='features',\n",
    "                   labelCol='class_index',\n",
    "                   predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14de63",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "### Create a pipeline model contains all the steps starting from the Tokenizer to the NaiveBays classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ee0d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, stringIndexer, vecAssembler, model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7efbe",
   "metadata": {},
   "source": [
    "### Split data to trian and test data with ratios 0.7 and 0.3 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2843d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df1.randomSplit([.7,.3],seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcea576",
   "metadata": {},
   "source": [
    "### Fit Pipeline model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c5d4681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/26 13:41:29 WARN DAGScheduler: Broadcasting large task binary with size 1044.6 KiB\n",
      "22/07/26 13:41:30 WARN DAGScheduler: Broadcasting large task binary with size 1026.4 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a3eb1",
   "metadata": {},
   "source": [
    "### Perform predictions on tests dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14f4aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce2885f",
   "metadata": {},
   "source": [
    "### Print the schema of the prediction dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e3ea341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- Tokenized_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Tokenized_words_no_stop_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- vectors: vector (nullable = true)\n",
      " |-- tfidf_features: vector (nullable = true)\n",
      " |-- class_index: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f27055",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "- Use <b>MulticlassClassificationEvaluator</b> to calculate the <b>f1_score</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61911086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be706565",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='class_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af1f9ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/26 13:41:31 WARN DAGScheduler: Broadcasting large task binary with size 1229.4 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score = 0.9727502290227267\n"
     ]
    }
   ],
   "source": [
    "print(f'F1-score = {evaluator.evaluate(pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
